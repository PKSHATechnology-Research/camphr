"""
This type stub file was generated by pyright.
"""

import plac
from pathlib import Path
from typing import Any, Optional
@plac.annotations(
    texts_loc=(
        "Path to JSONL file with raw texts to learn from, with text provided as the key 'text' or tokens as the "
        "key 'tokens'",
        "positional",
        None,
        str,
    ),
    vectors_model="Name or path to spaCy model with vectors to learn from",
    output_dir=("Directory to write models to on each epoch", "positional", None, str),
    width=("Width of CNN layers", "option", "cw", int),
    depth=("Depth of CNN layers", "option", "cd", int),
    cnn_window=("Window size for CNN layers", "option", "cW", int),
    cnn_pieces=("Maxout size for CNN layers. 1 for Mish", "option", "cP", int),
    use_chars=("Whether to use character-based embedding", "flag", "chr", bool),
    sa_depth=("Depth of self-attention layers", "option", "sa", int),
    bilstm_depth=("Depth of BiLSTM layers (requires PyTorch)", "option", "lstm", int),
    embed_rows=("Number of embedding rows", "option", "er", int),
    loss_func=(
        "Loss function to use for the objective. Either 'L2' or 'cosine'",
        "option",
        "L",
        str,
    ),
    use_vectors=("Whether to use the static vectors as input features", "flag", "uv"),
    dropout=("Dropout rate", "option", "d", float),
    batch_size=("Number of words per training batch", "option", "bs", int),
    max_length=(
        "Max words per example. Longer examples are discarded",
        "option",
        "xw",
        int,
    ),
    min_length=(
        "Min words per example. Shorter examples are discarded",
        "option",
        "nw",
        int,
    ),
    seed=("Seed for random number generators", "option", "s", int),
    n_iter=("Number of iterations to pretrain", "option", "i", int),
    n_save_every=("Save model every X batches.", "option", "se", int),
    init_tok2vec=(
        "Path to pretrained weights for the token-to-vector parts of the models. See 'spacy pretrain'. Experimental.",
        "option",
        "t2v",
        Path,
    ),
    epoch_start=(
        "The epoch to start counting at. Only relevant when using '--init-tok2vec' and the given weight file has been "
        "renamed. Prevents unintended overwriting of existing weight files.",
        "option",
        "es",
        int,
    ),
)
def pretrain(
    texts_loc,
    vectors_model,
    output_dir,
    width=...,
    depth=...,
    bilstm_depth=...,
    cnn_pieces=...,
    sa_depth=...,
    use_chars: bool = ...,
    cnn_window=...,
    embed_rows=...,
    loss_func=...,
    use_vectors: bool = ...,
    dropout=...,
    n_iter=...,
    batch_size=...,
    max_length=...,
    min_length=...,
    seed=...,
    n_save_every: Optional[Any] = ...,
    init_tok2vec: Optional[Any] = ...,
    epoch_start: Optional[Any] = ...,
):
    """
    Pre-train the 'token-to-vector' (tok2vec) layer of pipeline components,
    using an approximate language-modelling objective. Specifically, we load
    pretrained vectors, and train a component like a CNN, BiLSTM, etc to predict
    vectors which match the pretrained ones. The weights are saved to a directory
    after each epoch. You can then pass a path to one of these pretrained weights
    files to the 'spacy train' command.

    This technique may be especially helpful if you have little labelled data.
    However, it's still quite experimental, so your mileage may vary.

    To load the weights back in during 'spacy train', you need to ensure
    all settings are the same between pretraining and training. The API and
    errors around this need some improvement.
    """
    ...

def make_update(model, docs, optimizer, drop=..., objective=...):
    """Perform an update over a single batch of documents.

    docs (iterable): A batch of `Doc` objects.
    drop (float): The dropout rate.
    optimizer (callable): An optimizer.
    RETURNS loss: A float for the loss.
    """
    ...

def make_docs(nlp, batch, min_length, max_length): ...
def get_vectors_loss(ops, docs, prediction, objective=...):
    """Compute a mean-squared error loss between the documents' vectors and
    the prediction.

    Note that this is ripe for customization! We could compute the vectors
    in some other word, e.g. with an LSTM language model, or use some other
    type of objective.
    """
    ...

def create_pretraining_model(nlp, tok2vec):
    """Define a network for the pretraining. We simply add an output layer onto
    the tok2vec input model. The tok2vec input model needs to be a model that
    takes a batch of Doc objects (as a list), and returns a list of arrays.
    Each array in the output needs to have one row per token in the doc.
    """
    ...

class ProgressTracker(object):
    def __init__(self, frequency=...):
        self.loss = ...
        self.prev_loss = ...
        self.nr_word = ...
        self.words_per_epoch = ...
        self.frequency = ...
        self.last_time = ...
        self.last_update = ...
        self.epoch_loss = ...
    def update(self, epoch, loss, docs): ...

def _smart_round(figure, width=..., max_decimal=...):
    """Round large numbers as integers, smaller numbers as decimals."""
    ...
